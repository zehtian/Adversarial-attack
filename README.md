# Adversarial-attack
**此文件夹为对抗攻击实现代码，包括一些经典的对抗攻击方法和我们提出的新方法。**

对抗攻击的核心：找到一个对抗样本x'，s.t. f(x')≠f(x) & minimize(d(x',x))

也被认为是，使得模型发生预测错误需要的最小扰动r，r=d(x',x)。

其中，对于targeted attack，需要使得预测标签为设定的某个值，即f(x')=yk，对于non-targeted attack，只需要预测标签与原始标签不同即可，即f(x')≠f(x)。

## 1. 主要内容：测试各种对抗攻击方法。

主函数运行main.py 

环境：普通的机器学习训练环境。MNIST-Lenet; CIFAR10-Resnet18


## 2. 经典的对抗攻击方法
包括(name)：Deepfool, CW attack, boundary attack, hsja。见文件Adv_{name}.py

均为使用Advbox或toolbox进行对抗攻击，应用与我们联邦学习下的成员推断攻击的情况。


**Deepfool：**
基于梯度的对抗算法。白盒，目标/无目标对抗攻击方法。

朝着与原始样本最近的决策边界方向（该数据的梯度方向）进行更新，进行一定迭代次数后，当更新后的样本为对抗样本时，停止迭代，此时的对抗样本即为距离原始样本最近的对抗样本。


**CW attack：**
基于优化的对抗算法。白盒，目标/无目标对抗攻击方法。CW算法是目前最先进的对抗攻击算法。

修改了损失函数L，为两点距离与f(x')的线性组合，f(x)见论文。采用二分法迭代（steps）计算线性参数c，并采用Adam优化进行每一步固定迭代次数（max_iterations）的更新。得到使得扰动后的样本具有对抗性且使得损失函数L最小的扰动r。


**boundary attack：**
基于标签的对抗算法。黑盒，目标/无目标对抗攻击方法。

该算法从一个已经是对抗性的点初始化，然后沿着对抗性和非对抗性区域之间的边界执行随机游走，使得它停留在对抗性区域，且与目标图像的距离减少。当迭代得到的数据点越接近原始图像，决策边界越平坦，目标的小幅移动也越小。只要收敛到零，攻击就会收敛，同时停止迭代，此时的对抗样本与原始样本l2距离最短。


**hsja**
基于标签的对抗算法。黑盒，目标/无目标对抗攻击方法。hsja思想与我们的方法思想类似，均基于了几何关系。

HSJA为黑盒攻击，不需要直接利用模型的梯度信息，在决策边界处提出了一种新颖的梯度方向无偏估计，对该处的梯度进行了模拟，并提出了控制偏离边界误差的方法。

**算法概述**：1）初始化两个样本，分别为原始样本x(非对抗性)与任意扰动后的对抗性样本^xt；
2）进行边界搜索，找到本轮迭代样本xt；
3）梯度方向估计，估计该样本的梯度方向；
4）通过几何级数进行步长搜索，沿该梯度方向进行样本更新，并通过二分搜索进行边界搜索，得到^xt+1；
5）重复2-4，直至迭代结束，通过②中方法，得到最后样本^xt+i。


## 3. 我们提出的对抗攻击方法**
**MinAD**
为黑盒基于标签的攻击方法，见Adv_ours.py。


**动机：**
我们的方法其实可以理解为HSJA的改进，就是通过几何方法，在决策边界两侧来回跳动，直到得到最近的对抗样本。

Finding Optimal Tangent Points for Reducing Distortions of Hard-label Attacks论文指出其实HSJA在每轮迭代更新的方向并不是最优的方向，他们提出了Tangent attack，说明了以决策边界上点为圆心，原始数据点与该圆的切线方向才是最优方向：
 
但其实这个方法受到决策边界形状的影响很大，会导致圆的半径选取有误，同时不一定能很快收敛。

因此我们考虑将损失函数的优化方法与几何方法结合起来，为此，我们设置了特定的优化函数，将 决策边界上点与原始样本的连接向量 与 决策边界在该点上的法向量 的夹角的余弦相似度作为优化函数，这样得到的样本梯度更新方向一定是的角度减小最快的方向，也就是最优方向。


**Non-query attack**
改进了MinAD，不需要进行模型的查询，黑盒方法。见nq1 nq2.py，为两种方案。

第一种：修改原始的损失函数。对于迭代过程中某一位于决策边界上的样本，将该样本与原始样本之间向量与该样本与上次迭代时位于决策边界上的样本之间的向量的余弦相似度作为损失函数。经过测试该方法能够收敛，但容易提前收敛（陷入局部收敛），需要进行下一步完善。[这种方法较好理解，由于我们是采用优化损失函数的方法，因此不需要一定是用法向量作为参照，因此我们尝试了用连接向量]；


第二种：修改梯度估计方法。利用了决策边界较为平缓的特点。对于迭代过程中某一位于决策边界上的样本xi，同样取上一次迭代时位于决策边界上的样本xi-1，计算xi-1对于xi的对称点x*，通过二分法将x*投影到边界上得x*’，将xi- xi-1 与xi- x*’之和作为xi处的梯度估计，此方法准备下周验证[不同情况：1.决策边界为圆弧，二者之和就是梯度；2. 决策边界为直线，二者之和为0；3. 当决策边界较为缓慢且在xi-1~ x*’范围内为凸的情况下，我觉得可以把二者之和当作梯度估计]。
